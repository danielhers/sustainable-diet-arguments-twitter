{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import tweepy\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Twitter stuff\n",
    "credentials_path = './../credentials.json'\n",
    "\n",
    "with io.open(credentials_path) as f_in:\n",
    "    credentials = json.load(f_in)\n",
    "\n",
    "\n",
    "access_token = credentials[\"access_token\"]\n",
    "access_token_secret = credentials[\"access_token_secret\"]\n",
    "\n",
    "\n",
    "api_key = credentials[\"api_key\"]\n",
    "api_secret = credentials[\"api_secret\"]\n",
    "bearer_token = credentials[\"bearer_token\"]\n",
    "\n",
    "consumer_key = api_key\n",
    "consumer_secret = api_secret\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "seed       = 42\n",
    "model_name = \"bert-base-german-cased\"\n",
    "task = 'argumentative_expert2'#'claim_expert1'#'evidence_expert1' # argumentative_expert1\n",
    "metric     = load_metric('accuracy')\n",
    "epochs     = 1\n",
    "df_path = './climate_twitter_tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # If you don't already have scraped the tweets switch this to true\n",
    "    df = pd.read_csv('./global_expert_annotations.csv')\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    ids = df['tweet_id']\n",
    "    tweets = []\n",
    "    for id_ in ids:\n",
    "        try:\n",
    "            tweet = api.get_status(int(id_), tweet_mode='extended')\n",
    "            tweets.append(tweet._json)\n",
    "        except:\n",
    "            pass\n",
    "        print(len(tweets))\n",
    "\n",
    "        if len(tweets) % 5000 == 0:\n",
    "            dir_name = os.path.dirname(out_dir)\n",
    "            file_path = os.path.join(dir_name, 'tweets_' + str(len(tweets)) + '.json')\n",
    "            with io.open(file_path, mode='w') as f_out:\n",
    "                json.dump(tweets,f_out)               \n",
    "\n",
    "    df = df.assign(tweet=pd.Series(['']*len(df)).values)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        df.loc[tweet['id'] == df.tweet_id, 'tweet'] = tweet['full_text']\n",
    "    df.to_csv(df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(df_path)\n",
    "## Remove empty strings\n",
    "df = df[df.tweet != '']\n",
    "df = df[df.tweet.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init embedding\n",
    "embedding = TransformerDocumentEmbeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_embeddings = []\n",
    "\n",
    "for tweet in df.tweet:\n",
    "    tweet_embeddings.append(embedding.embed(Sentence(tweet))[0].get_embedding().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(tweet_embeddings)\n",
    "label = df[task].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRFClassifier(n_estimators=1, max_depth=1, objective='binary:logistic', eval_metric='auc', tree_method=\"gpu_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    #'gamma': [0,0.1,0.2,0.4,0.8,1.0],\n",
    "    'learning_rate': [0.01, 0.03, 0.06],\n",
    "    'max_depth': [1,3,5,6,7,8,9,10],\n",
    "    'n_estimators': [1,2,5,7,10,15,20,25,30],\n",
    "    #'reg_alpha': [0,0.1,0.2,0.4,0.8,1.0],\n",
    "    #'reg_lambda': [0,0.1,0.2,0.4,0.8,1.0],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric':['auc'], \n",
    "    'tree_method':[\"gpu_hist\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf0 = GridSearchCV(estimator=model, scoring='f1_macro', param_grid=param_grid, n_jobs=10, verbose=1, cv=3)\n",
    "#clf0.fit(data, label)\n",
    "#df = pd.DataFrame(clf0.cv_results_)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mah/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRFClassifier(n_estimators=15, max_depth=1, learning_rate=0.01, objective='binary:logistic', eval_metric='auc', tree_method=\"gpu_hist\")\n",
    "cv_results = cross_validate(model, data, label, scoring=('f1_weighted', 'precision', 'recall'), cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8400990607537462, 0.8866704640388852, 0.9777777777777779)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_f1_weighted'].mean(), cv_results['test_precision'].mean(),cv_results['test_recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bynode': 0.8,\n",
       " 'learning_rate': 0.01,\n",
       " 'reg_lambda': 1e-05,\n",
       " 'subsample': 0.8,\n",
       " 'use_label_encoder': True,\n",
       " 'objective': 'binary:logistic',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bytree': None,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 1,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 15,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None,\n",
       " 'eval_metric': 'auc'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
