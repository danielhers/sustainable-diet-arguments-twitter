{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from utils import numerical_df\n",
    "import re\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_path = './annotated-dataset.csv'\n",
    "df_org = pd.read_csv(df_path)\n",
    "## Remove empty strings\n",
    "df = df_org[df_org.tweet != '']\n",
    "df = df[df.tweet.notnull()]\n",
    "\n",
    "df.loc[df.argumentative >= 0.5, 'argumentative'] = 1\n",
    "df.loc[df.argumentative < 0.5, 'argumentative'] = 0\n",
    "\n",
    "df.loc[df.claim >= 0.5, 'claim'] = 1\n",
    "df.loc[df.claim < 0.5, 'claim'] = 0\n",
    "\n",
    "df.loc[df.evidence < 0.5, 'evidence'] = 0\n",
    "df.loc[df.evidence >= 0.5, 'evidence'] = 1\n",
    "\n",
    "\n",
    "df.loc[df.procon < 0, 'procon'] = -1\n",
    "df.loc[df.procon > 0, 'procon'] = 1\n",
    "\n",
    "num = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(string, lang = 'en'):\n",
    "    stemmer = SnowballStemmer('english' if lang == 'en' else 'danish')\n",
    "    stops = set(stopwords.words('english' if lang == 'en' else 'danish'))\n",
    "    \n",
    "    words = word_tokenize(re.sub('[^a-zA-Z]', ' ', string.lower().strip()))\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    return ' '.join([stemmer.stem(w) for w in meaningful_words])\n",
    "\n",
    "def mean_topic_tweet_over_lap(df):\n",
    "    tt = df.tweet.map(pre).map(str.split).map(np.unique)\n",
    "    t = df.topic.map(pre).map(str.split).map(np.unique)\n",
    "    return np.mean(list(map(lambda xy: len(np.intersect1d(xy[0], xy[1]))/len(np.union1d(xy[0], xy[1])), zip(tt, t))))\n",
    "\n",
    "def tweet_overlap(df):\n",
    "    v = df.tweet.drop_duplicates().map(pre).map(str.split).map(np.unique)\n",
    "    return np.mean(list(map(lambda xy: len(np.intersect1d(xy[0], xy[1]))/len(np.union1d(xy[0], xy[1])), itertools.combinations(v, 2))))\n",
    "\n",
    "def generate_word_cloud(df, topic = None, label = None):\n",
    "    text = \" \".join(i.replace('<MENTION>', '') for i in df.drop_duplicates().tweet)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=1000, height=500).generate(text)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(wordcloud)#, interpolation='bilinear')\n",
    "    topic_str = '' if topic == None else f'for topic \\n \"{topic}\"'\n",
    "    label_str = '' if label == None else f'for label \"{label}\"'\n",
    "    plt.title(f'Wordcloud of tweets {label_str}, {topic_str}')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_word_hist(df, topic = None, label = None):\n",
    "    rm_mention = lambda x: str.replace(x, '<MENTION>', '')\n",
    "    text = np.concatenate([i.split(' ') for i in df.tweet.drop_duplicates().map(rm_mention).map(pre)])\n",
    "\n",
    "    counts = Counter(text)\n",
    "\n",
    "    labels, values = zip(*counts.items())\n",
    "    top = 10\n",
    "    # sort your values in descending order\n",
    "    indSort = np.argsort(values)[::-1][:top]\n",
    "\n",
    "    # rearrange your data\n",
    "    labels = np.array(labels)[indSort]\n",
    "    values = np.array(values)[indSort]/sum(values)*100\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.bar(indexes, values)\n",
    "\n",
    "    # add labels\n",
    "    plt.xticks(indexes + bar_width, labels, rotation=45)\n",
    "    plt.xlabel(f'Top {top} words used in corpus')\n",
    "    plt.ylabel('Proportion of corpus %')\n",
    "    \n",
    "    topic_str = '' if topic == None else f'for topic \\n \"{topic}\", '\n",
    "    label_str = '' if label == None else f'with label \"{label}\"'\n",
    "    plt.title(f'Distribution of words used in tweets {label_str}{topic_str}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def df_info(df, topic = None, gen_word_cloud = True, gen_hist = True):\n",
    "    print('Total documents:', len(df), 'of which is argumentative:', sum(df.argumentative))\n",
    "    print('ADUs', len(df[(df.claim > 0) | (df.evidence > 0)]), 'claims:', sum(df.claim), 'evidence:', sum(df.evidence), 'claim with evidence', len(df[(df.claim > 0) & (df.evidence > 0)]))\n",
    "    print('Support:', sum(df[df.procon > 0].procon), 'Contest:', len(df[df.procon < 0]))\n",
    "    \n",
    "    print('Mean tweet length:', df.tweet.map(str.split).map(len).mean())\n",
    "    print('Mean claim length:', df[df.claim > 0].tweet.map(str.split).map(len).mean())\n",
    "    print('Mean evidence length:', df[df.evidence > 0].tweet.map(str.split).map(len).mean())\n",
    "    print()\n",
    "    print('Mean topic tweet vocab share %:', mean_topic_tweet_over_lap(df))\n",
    "    print('Mean claim, topic tweet vocab share %:', mean_topic_tweet_over_lap(df[df.claim > 0]))\n",
    "    print('Mean evidence, topic tweet vocab share %:', mean_topic_tweet_over_lap(df[df.evidence > 0]))\n",
    "    print()\n",
    "    print('Mean claim tweet to tweet vocab overlap %:', tweet_overlap(df[df.claim > 0]))\n",
    "    print('Mean evidence tweet to tweet vocab overlap %:', tweet_overlap(df[df.evidence > 0]))\n",
    "    \n",
    "    if gen_word_cloud:\n",
    "        generate_word_cloud(df[df.argumentative > 0], topic, 'argumentative')\n",
    "        generate_word_cloud(df[df.claim > 0], topic, 'claim')\n",
    "        generate_word_cloud(df[df.evidence > 0], topic, 'evidence')\n",
    "        generate_word_cloud(df[df.procon > 0], topic, 'pro')\n",
    "        generate_word_cloud(df[df.procon < 0], topic, 'con')\n",
    "\n",
    "    \n",
    "    if gen_hist:\n",
    "        generate_word_hist(df, topic)\n",
    "        generate_word_hist(df[df.argumentative > 0], topic, 'argumentative')\n",
    "        generate_word_hist(df[df.claim > 0], topic, 'claim')\n",
    "        generate_word_hist(df[df.evidence > 0], topic, 'evidence')\n",
    "        generate_word_hist(df[df.procon > 0], topic, 'pro')\n",
    "        generate_word_hist(df[df.procon < 0], topic, 'con')\n",
    "        \n",
    "    return len(df), sum(df.argumentative), len(df[(df.claim > 0) | (df.evidence > 0)]), sum(df.claim), sum(df.evidence), len(df[(df.claim > 0) & (df.evidence > 0)]), sum(df[df.procon > 0].procon), len(df[df.procon < 0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in df.topic.drop_duplicates():\n",
    "    print(topic)\n",
    "    generate_word_cloud(df, topic, 'claim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df.topic.drop_duplicates()\n",
    "task = 'claim'\n",
    "hex_map = []\n",
    "for topicX in topics:\n",
    "    inst = []\n",
    "    for topicY in topics:\n",
    "        subset = df[((df.topic == topicX) | (df.topic == topicY)) & ((df[task] > .5))]\n",
    "        r = np.round(sum(subset.groupby(['tweet']).size() == 2) / len(subset.tweet.drop_duplicates()), 2)\n",
    "        if r == 0:\n",
    "            inst.append(1.0)\n",
    "        else:\n",
    "            inst.append(r)\n",
    "    hex_map.append(inst)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "im = ax.imshow(hex_map)\n",
    "hex_map = np.array(hex_map)\n",
    "labels = [\n",
    "    'meat',\n",
    "    'plant',\n",
    "    'alternative',\n",
    "    'vegan',\n",
    "    'policy'\n",
    "]\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(topics)), labels=labels)\n",
    "ax.set_yticks(np.arange(len(topics)), labels=labels)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        text = ax.text(j, i, hex_map[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "#ax.set_title(f\"Percentage of tweets overlapping with other topics for {task}\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(f'topic_overlap_{task}.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info(df, None, False, True) # Total info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in df.topic.drop_duplicates():\n",
    "    print('Topic', topic)\n",
    "    df_info(df[df.topic == topic], topic, False, False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = chr(92)\n",
    "br = lambda x: '{' + x + '}'\n",
    "r = lambda x: round(x, 2)\n",
    "row = lambda topic, num_tweet, num_arg, num_adu, num_claim, num_evi, num_claim_evi, num_pro, num_con: f'''\n",
    "        {bc}hline\n",
    "        {topic} & ${r(num_tweet)}$ & ${r(num_arg)}$ & ${r(num_adu)}$ & ${r(num_claim)}$ & ${r(num_evi)}$ & ${r(num_claim_evi)}$ & ${r(num_pro)}$ & ${r(num_con)}$ {bc}{bc}\n",
    "    '''\n",
    "rows = [row('Full set', *df_info(df, None, False, False)), *[row(topic, *df_info(df[df.topic == topic], topic, False, False)) for topic in df.topic.drop_duplicates()]]\n",
    "\n",
    "print(f'''\n",
    "{bc}begin{br(\"table\")}[H]\n",
    "    {bc}centering\n",
    "    {bc}begin{br(\"tabular\")}{br(\"L|ccc|ccc|cc\")}\n",
    "        Topic & Tweets & Argumentative & ADUs & Claims & Evidence & Claims with evidence & Pro & Con  {bc}{bc}\n",
    "        {(bc + \"n\").join(rows)}\n",
    "    {bc}end{br(\"tabular\")}\n",
    "    {bc}caption{br(\"Overall statistics comparison for the different topics and the overall full set. ADUs is argument discourse units and is a union of claim or evidence and says how many argumentative tweets contains either evidence or claims. The probabilities have been rounded to the nearest integer for this comparison.\")}\n",
    "    {bc}label{br(\"table:overall_stats_of_data\")}\n",
    "{bc}end{br(\"table\")}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "evi_dist = {}\n",
    "\n",
    "for i, evi in enumerate(df.evidence_type):\n",
    "    if not pd.isna(evi):\n",
    "        evi = ast.literal_eval(evi)\n",
    "        for key in evi:\n",
    "            if key in evi_dist:\n",
    "                evi_dist[key] += evi[key]\n",
    "            else:\n",
    "                evi_dist[key] = evi[key]\n",
    "                \n",
    "\n",
    "labels, values = zip(*evi_dist.items())\n",
    "\n",
    "# rearrange your data\n",
    "labels = np.array(labels)\n",
    "values = np.array(values)/sum(values)*100\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "\n",
    "# add labels\n",
    "plt.xticks(indexes + bar_width, labels, rotation=45, ha=\"right\")\n",
    "plt.xlabel(f'Evidence types used in corpus')\n",
    "plt.ylabel('Proportion of evidence labels %')\n",
    "\n",
    "\n",
    "plt.title(f'Evidence types in corpus')\n",
    "\n",
    "plt.savefig('evidence_types.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
