{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui asyncio\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import tweepy\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "import nltk\n",
    "import asyncio\n",
    "from ipywidgets import widgets as w\n",
    "from IPython.display import clear_output\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seed       = 42\n",
    "model_name = \"bert-base-cased\"\n",
    "#task = 'claim_expert1'#'evidence_expert1' # argumentative_expert1\n",
    "#metric     = load_metric('accuracy')\n",
    "epochs     = 1\n",
    "df_path = './scraped_tweets.csv'\n",
    "\n",
    "load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(df_path)\n",
    "## Remove empty strings\n",
    "df = df[df.tweet != '']\n",
    "df = df[df.tweet.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load:\n",
    "    data = np.load('./scraped_tweets_embeddings.npy', allow_pickle=True)\n",
    "else:\n",
    "    # init embedding\n",
    "    embedding = TransformerDocumentEmbeddings(model_name, embeddings_storage_mode='none')\n",
    "    tweet_embeddings = []\n",
    "\n",
    "    for i, tweet in enumerate(df.tweet):\n",
    "        print('\\r', i, end='')\n",
    "        tweet_embeddings.append((embedding.embed(Sentence(tweet))[0].get_embedding().cpu().detach().numpy(), tweet))\n",
    "\n",
    "    data = np.array(tweet_embeddings, dtype=object)\n",
    "    np.save('scraped_tweets_embeddings', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_table(numRows):\n",
    "    rows = []\n",
    "    \n",
    "    header = w.HTML()\n",
    "    done = w.Button(\n",
    "        description='Done',\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Click if done with annotation',\n",
    "        icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    "    )\n",
    "    \n",
    "\n",
    "    gs = w.GridspecLayout(numRows+2, 2)\n",
    "    gs[0,0] = header\n",
    "    gs[numRows+1, 0] = done\n",
    "    \n",
    "    for i in range(numRows):\n",
    "        btn = w.ToggleButtons(\n",
    "            options=[0, 1],\n",
    "            description='Label:',\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "        )\n",
    "    \n",
    "        tweet = w.HTML()\n",
    "        vector = {'value': ''} # This happens to save the tweet vector\n",
    "        rows.append((tweet, btn, vector))\n",
    "        \n",
    "        gs[i+1, 0] = tweet\n",
    "        gs[i+1, 1] = btn\n",
    "    \n",
    "    return gs, rows, header, done\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_filter(data):\n",
    "    gs, rows, header, done = gen_table(10)\n",
    "    hasRun = False\n",
    "    train = {\"x\": [], \"y\": []}\n",
    "\n",
    "    def combined(_):\n",
    "        save()\n",
    "        iterate()\n",
    "\n",
    "    def save():\n",
    "        nonlocal train\n",
    "        nonlocal header\n",
    "        nX = []\n",
    "        nY = []\n",
    "        for row in rows:\n",
    "            nX.append((row[2]['value'], row[0].value)) # Vector / Tweet\n",
    "            nY.append(row[1].value) # Label\n",
    "\n",
    "        if len(train['x']) > 0:\n",
    "            train['x'] = np.concatenate((train['x'], nX))\n",
    "            train['y'] = np.concatenate((train['y'], nY))\n",
    "        else:\n",
    "            train['x'] = np.array(nX, dtype=object)\n",
    "            train['y'] = np.array(nY)\n",
    "        header.value = f'Number of positively found tweets: {sum(train[\"y\"])}'\n",
    "        \n",
    "\n",
    "    def iterate():\n",
    "        nonlocal train\n",
    "        nonlocal data\n",
    "        if len(train['x']) > 0:\n",
    "\n",
    "            model = xgb.XGBRFClassifier(n_estimators=10, max_depth=2, learning_rate=0.01, objective='binary:logistic', eval_metric='logloss', tree_method=\"gpu_hist\")\n",
    "            model.fit(np.array(list(train['x'][:,0]), dtype=np.float), train['y'])\n",
    "\n",
    "            converted = np.array(list(data[:,0]), dtype=np.float) \n",
    "            probs = model.predict_proba(converted)[:,1].argsort()\n",
    "            preds = model.predict(converted)\n",
    "        else:\n",
    "            preds = np.zeros(len(data))\n",
    "            probs = np.random.permutation(len(data))\n",
    "\n",
    "        bad_sort = probs[:5] # Low prediction\n",
    "        good_sort = probs[-5:] # High prediciton\n",
    "        sort = np.concatenate((good_sort, bad_sort))\n",
    "\n",
    "        dt = data[sort]\n",
    "        dp = preds[sort]\n",
    "\n",
    "        for i, (tweet, prediction) in enumerate(zip(dt, dp)):\n",
    "            rows[i][0].value = tweet[1] # Sets the tweet text\n",
    "            rows[i][1].value = prediction # Sets the label\n",
    "            rows[i][2]['value'] = tweet[0] # Saves the vector \n",
    "\n",
    "        data = np.delete(data, sort, axis = 0)\n",
    "\n",
    "\n",
    "    done.on_click(combined)\n",
    "    iterate() # We iterate once to pull in the first set of tweets\n",
    "    return gs, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs, annotated = annotate_filter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffc9291f6e346cbb49f267881c58c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(HTML(value='', layout=Layout(grid_area='widget001')), Button(description='Done', iconâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([annotated['x'][:,1], annotated['y']]).T)\n",
    "df.columns = ['tweet', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('filtered_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
