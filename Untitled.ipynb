{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_argumentative():\n",
    "    df = pd.read_csv('Ibm-datasets/argumentative/set.csv')\n",
    "    df = df.drop(columns = ['#positive', '#negative', 'sentence_internal_id', 'val', 'test'])\n",
    "    df.columns = ['id', 'argument', 'tweet', 'topic', 'argumentative']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_evidence():\n",
    "    df = pd.concat([pd.read_csv('Ibm-datasets/evidence/test.csv'), pd.read_csv('Ibm-datasets/evidence/train.csv')])\n",
    "    df = df.drop(columns = ['the concept of the topic', 'candidate masked', 'wikipedia article name', 'wikipedia url'])\n",
    "    df.columns = ['topic', 'tweet', 'evidence']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibm_procon():\n",
    "    df = pd.read_csv('Ibm-datasets/procon/set.csv')\n",
    "    df = df.drop(columns = ['split', 'topicTarget', 'topicSentiment', 'claims.claimId', 'claims.claimCorrectedText', 'claims.article.rawFile', 'claims.article.cleanFile', 'claims.article.rawSpan.start', 'claims.article.rawSpan.end', 'claims.article.cleanSpan.start', 'claims.article.cleanSpan.end', 'claims.Compatible','claims.claimSentiment', 'claims.targetsRelation', 'claims.claimTarget.span.end', 'claims.claimTarget.span.start', 'claims.claimTarget.text'])\n",
    "    df.columns = ['id','topic', 'procon', 'tweet']\n",
    "    df.procon = df.procon.apply(lambda x: 1 if x == 'PRO' else -1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score as f1, precision_score as ps, recall_score as rs\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_tasks = ['argumentative', 'claim', 'evidence','procon'] \n",
    "\n",
    "def bert_baseline(df_train, df_test, model_name='bert-base-cased', tasks = possible_tasks, use_topic = True, seed = 42, batch_size = 5, epochs = 1):\n",
    "    \n",
    "    \n",
    "    # Make sure task is correctly formatted\n",
    "    if not isinstance(tasks, str) and not isinstance(tasks, list):\n",
    "        raise ValueError(\"task must be list or str\")\n",
    "    \n",
    "    if type(tasks) == str:\n",
    "        tasks = [tasks]\n",
    "\n",
    "    if not all(elem in possible_tasks for elem in tasks):\n",
    "        raise ValueError(\"task must only contain any of the following strings: \", possible_tasks, ', but found:', tasks)\n",
    "        \n",
    "    metric = load_metric('accuracy')\n",
    "    \n",
    "    print('Loading tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def tokenize_function(examples):\n",
    "        if use_topic: \n",
    "            tweet = f\"{examples['topic']} [SEP] {examples['tweet']}\"\n",
    "        else:\n",
    "            tweet = examples['tweet']\n",
    "        \n",
    "        return tokenizer(tweet, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    res = [] # Columns\n",
    "    \n",
    "    for task in tasks:\n",
    "        print('Loading language model')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        \n",
    "        print('Setting up dataset for', task)\n",
    "        df_train_renamed = df_train.rename(columns={task: \"labels\"})\n",
    "        df_test_renamed = df_test.rename(columns={task: \"labels\"})\n",
    "        \n",
    "        train_data = Dataset.from_pandas(df_train_renamed)\n",
    "        test_data  = Dataset.from_pandas(df_test_renamed)\n",
    "        \n",
    "        \n",
    "        train_processed = train_data.map(tokenize_function).map(lambda x: x, batched=True, batch_size = batch_size)\n",
    "        test_processed = test_data.map(tokenize_function).map(lambda x: x, batched=True, batch_size = batch_size)\n",
    "        \n",
    "        train_full = train_processed.shuffle(seed=seed).remove_columns( df_train_renamed.loc[:, df_train_renamed.columns != 'labels'].columns)\n",
    "        test_full  = test_processed.shuffle(seed=seed).remove_columns( df_test_renamed.loc[:, df_test_renamed.columns != 'labels'].columns)\n",
    "        \n",
    "        \n",
    "        print('Setting up training')\n",
    "        \n",
    "        train_args = TrainingArguments(\n",
    "            output_dir = model_name + '_' + task,\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            num_train_epochs = epochs,\n",
    "            evaluation_strategy='epoch'\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model = model,\n",
    "            args = train_args,\n",
    "            train_dataset = train_full,\n",
    "            eval_dataset = test_full,\n",
    "            compute_metrics = compute_metrics,\n",
    "        )\n",
    "        \n",
    "        print('Training')\n",
    "        trainer.train()\n",
    "        \n",
    "        print('Inferring')\n",
    "        predictions = trainer.predict(test_full)\n",
    "        preds = predictions.predictions.argmax(-1)\n",
    "        \n",
    "        label = df_test[task].to_numpy()\n",
    "        #print(label, test_full['labels'], predictions.predictions, preds)\n",
    "        \n",
    "        res.append((task, f1(label, preds, average='micro'), ps(label, preds, average='micro'), rs(label, preds, average='micro')))\n",
    "    \n",
    "    res = pd.DataFrame(res)\n",
    "    res.columns = ('Tasks','F1', 'Precision', 'Recall')\n",
    "    res = res.style.set_caption('Results finetuning bert-model ' + model_name + ' on ' + task + ' dataset and applying to test set')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ibm_evidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_baseline(df, df, tasks = 'evidence', epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
