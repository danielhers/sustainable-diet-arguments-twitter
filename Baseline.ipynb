{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import tweepy\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import nltk\n",
    "from utils import numerical_df\n",
    "from baselines import xgboost_baseline, majority_class_baseline, random_class_baseline, ibm_baseline, bert_baseline, bm25_baseline, qa_model_baseline\n",
    "from sklearn.metrics import f1_score as f1, precision_score as ps, recall_score as rs\n",
    "from sklearn.model_selection import KFold\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = './annotated-dataset.csv'\n",
    "df_org = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove empty strings\n",
    "df = df_org[df_org.tweet != '']\n",
    "df = df[df.tweet.notnull()]\n",
    "\n",
    "#bert_baseline(df, tasks=current_classes, use_topic=True, batch_size=5, epochs = 1)\n",
    "#display(ibm_baseline(df_arg, ['argumentative']))\n",
    "df_temp = df.reset_index(drop=True)\n",
    "df_temp['tweet'] =  df_temp.tweet.map(lambda x: x.replace('<MENTION>', '@user'))\n",
    "display(bert_baseline(df_temp, \n",
    "                      fold = 3,\n",
    "                      model_name = 'cardiffnlp/twitter-roberta-base', \n",
    "                      tasks = ['argumentative', 'claim', 'evidence', 'procon'], \n",
    "                      learning_rate = 5e-5, \n",
    "                      batch_size=7,\n",
    "                      epochs = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove empty strings\n",
    "df = df_org[df_org.tweet != '']\n",
    "df = df[df.tweet.notnull()]\n",
    "\n",
    "df.loc[df.argumentative >= 0.5, 'argumentative'] = 1\n",
    "df.loc[df.argumentative < 0.5, 'argumentative'] = 0\n",
    "\n",
    "df.loc[df.claim >= 0.5, 'claim'] = 1\n",
    "df.loc[df.claim < 0.5, 'claim'] = 0\n",
    "\n",
    "df.loc[df.evidence < 0.5, 'evidence'] = 0\n",
    "df.loc[df.evidence >= 0.5, 'evidence'] = 1\n",
    "\n",
    "\n",
    "df.loc[df.procon < 0, 'procon'] = -1\n",
    "df.loc[df.procon > 0, 'procon'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.argumentative), sum(df.claim), sum(df.evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_classes = ['argumentative', 'claim', 'evidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(majority_class_baseline(df, ['argumentative', 'claim', 'evidence', 'procon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "possible_tasks = ['argumentative', 'claim', 'evidence','procon'] \n",
    "def calc_scores(preds_set, labels_set, task = None):\n",
    "    \n",
    "    average_sets = []\n",
    "    \n",
    "    for average in ['binary', 'macro', 'micro']:\n",
    "        averages = []\n",
    "        for preds, labels in zip(preds_set, labels_set):\n",
    "            averages.append((f1(preds, labels, average=average, zero_division = 0), \n",
    "                             ps(preds, labels, average=average, zero_division = 0), \n",
    "                             rs(preds, labels, average=average, zero_division = 0)))\n",
    "        average_sets.append((average, *np.round(np.mean(averages, axis=0), 2)))\n",
    "    \n",
    "    if task == None:\n",
    "        return np.array(average_sets)\n",
    "    else:\n",
    "        return np.hstack([[[task]]*3, np.array(average_sets)])\n",
    "\n",
    "\n",
    "def dummy_class_baseline(df, tasks = possible_tasks, fold = 10, strategy=\"stratified\"):\n",
    "    \n",
    "    if isinstance(tasks, str):\n",
    "        tasks = [tasks]\n",
    "    \n",
    "    if not isinstance(tasks, list):\n",
    "        return ValueError('Tasks is not a string or a list.')\n",
    "    \n",
    "    if not all(item in possible_tasks for item in tasks):\n",
    "        return ValueError(f'Tasks can only be or contain the following elements {possible_tasks} but found {tasks}')\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        kf = KFold(n_splits=fold)\n",
    "        df_use = df.copy()\n",
    "        data = np.array(df[task].values)#.round().astype(int)\n",
    "        if task == 'procon':\n",
    "            mask = data != 0\n",
    "            data = [d for d, m in zip(data, mask) if m]\n",
    "        elif task != 'argumentative':\n",
    "            mask = df_use.argumentative > .5\n",
    "            data = [d for d, m in zip(data, mask) if m]\n",
    "        else:\n",
    "            df_use = df_use.drop_duplicates(subset=['tweet'])\n",
    "            data = df_use.argumentative.to_numpy()\n",
    "        \n",
    "        data = np.array(data).round().astype(int)\n",
    "        tres = [] \n",
    "        tlabels = []\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            \n",
    "            dummy_clf = DummyClassifier(strategy=strategy)\n",
    "            X_train, X_test = data[train_index], data[test_index]  \n",
    "            dummy_clf.fit(X_train, X_train)\n",
    "\n",
    "            labels = dummy_clf.predict(X_test)\n",
    "            \n",
    "            tres.append(labels)\n",
    "            tlabels.append(X_test)\n",
    "            \n",
    "        res.append(calc_scores(tres, tlabels, task))\n",
    "    res = pd.DataFrame(np.concatenate(res))\n",
    "    res.columns = ['Task', 'Averaging', 'F1', 'Precision', 'Recall']\n",
    "    return res\n",
    "\n",
    "def random_class_baseline(df, tasks = possible_tasks, fold = 10):\n",
    "    res = dummy_class_baseline(df, tasks, fold, strategy = 'stratified')\n",
    "    res = res.style.set_caption(f'Random class results with {fold} fold split using weighted approach')\n",
    "    return res\n",
    "\n",
    "def majority_class_baseline(df, tasks = possible_tasks, fold = 10):\n",
    "    \n",
    "    res = dummy_class_baseline(df, tasks, fold, strategy = 'prior')\n",
    "    res = res.style.set_caption(f'Majority class results with {fold} fold split')\n",
    "    return res\n",
    "\n",
    "majority_class_baseline(df, ['argumentative', 'claim', 'evidence', 'procon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(random_class_baseline(df, ['argumentative', 'claim', 'evidence', 'procon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.reset_index(drop=True)\n",
    "df_temp['tweet'] =  df_temp.tweet.map(lambda x: x.replace('<MENTION>', '@user'))\n",
    "display(xgboost_baseline(df_temp, \n",
    "                         fold = 3, \n",
    "                         model_name = 'cardiffnlp/twitter-roberta-base', \n",
    "                         tasks = ['argumentative', 'claim', 'evidence', 'procon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25_baseline(df, current_classes)\n",
    "#display(bm25_baseline(df_arg, ['argumentative']))\n",
    "display(bm25_baseline(df, ['claim', 'evidence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_baseline(df.copy(), ['claim', 'evidence'], model_cutoff = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from utils import numerical_df\n",
    "from sklearn.metrics import f1_score as f1, precision_score as ps, recall_score as rs\n",
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import pipeline\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import json\n",
    "import io\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "possible_tasks = ['argumentative', 'claim', 'evidence','procon'] \n",
    "\n",
    "\n",
    "\n",
    "def calc_scores(preds_set, labels_set, task = None):\n",
    "    \n",
    "    average_sets = []\n",
    "    \n",
    "    for average in ['binary', 'macro', 'micro']:\n",
    "        averages = []\n",
    "        for preds, labels in zip(preds_set, labels_set):\n",
    "            averages.append((f1(preds, labels, average=average, zero_division = 0), \n",
    "                             ps(preds, labels, average=average, zero_division = 0), \n",
    "                             rs(preds, labels, average=average, zero_division = 0)))\n",
    "\n",
    "        average_sets.append((average, *np.round(np.mean(averages, axis=0), 2)))\n",
    "    \n",
    "    if task == None:\n",
    "        return np.array(average_sets)\n",
    "    else:\n",
    "        return np.hstack([[[task]]*3, np.array(average_sets)])\n",
    "\n",
    "\n",
    "def ibm_baseline(df, tasks = possible_tasks):\n",
    "    if not isinstance(tasks, str) and not isinstance(tasks, list):\n",
    "        raise ValueError(\"task must be list or str\")\n",
    "    \n",
    "    if type(tasks) == str:\n",
    "        tasks = [tasks]\n",
    "\n",
    "    if not all(elem in possible_tasks for elem in tasks):\n",
    "        raise ValueError(\"task must only contain any of the following strings: \", possible_tasks, ', but found:', tasks)\n",
    "    \n",
    "\n",
    "    credentials_path = './credentials.json'\n",
    "\n",
    "    with io.open(credentials_path) as f_in:\n",
    "        credentials = json.load(f_in)\n",
    "    \n",
    "    api_key = credentials['debater_api_key']\n",
    "    debater_api = DebaterApi(apikey=api_key)\n",
    "    clients = {\n",
    "        \"claim\": debater_api.get_claim_detection_client(),\n",
    "        \"evidence\": debater_api.get_evidence_detection_client(),\n",
    "        \"procon\": debater_api.get_pro_con_client(),\n",
    "        \"argumentative\": debater_api.get_argument_quality_client(),\n",
    "    }\n",
    "    \n",
    "    argumentative_mask = df.argumentative == 1\n",
    "\n",
    "    sentence_topic_dicts = [{'sentence' : row.tweet, 'topic' : row.topic } for row in df.iloc]\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        print('Gathering results for', task)\n",
    "        data = sentence_topic_dicts\n",
    "        label = df[task].to_numpy()\n",
    "        \n",
    "        client = clients[task]\n",
    "        \n",
    "        if task == 'procon':\n",
    "            mask = label != 0\n",
    "            data = [d for d, m in zip(data, mask) if m]\n",
    "            label = label[mask]\n",
    "            scores = [1 if s > 0 else -1 for s in client.run(data)]\n",
    "        else:\n",
    "            data = [d for d, m in zip(data, argumentative_mask) if m]\n",
    "            label = label[argumentative_mask]\n",
    "            r = client.run(data)\n",
    "            #scores = np.round(r)\n",
    "            scores = [1 if s >= .5 else 0 for s in r]\n",
    "            tp = (label == 1) & (scores == label)\n",
    "            fn = (label == 1) & (scores != label)\n",
    "            fp = (label == 0) & (scores != label)\n",
    "            recall = sum(tp)/(sum(label))\n",
    "            precision = sum(tp)/(sum(tp) + sum(fp))\n",
    "            print(sum(scores), sum(label), recall, precision)\n",
    "        \n",
    "        res.append(calc_scores([scores], [label], task))\n",
    "    res = pd.DataFrame(np.concatenate(res))\n",
    "    res.columns =  ['Task', 'Averaging', 'F1', 'Precision', 'Recall']\n",
    "    res = res.style.set_caption('Results from using imbs project debater api for 0 shot evalutaion')\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibm_baseline(df, ['evidence']) # Only run this if you have access to ibms api models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
